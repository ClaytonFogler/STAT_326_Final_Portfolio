---
title: "Mini Project Reflection"
date: "05-06-2025"
author: "Clayton Fogler"
---

Throughout the semester, we came across 5 mini projects that not only tie together with each other but with the materials we have learned throughout the semester.

In mini project 1, we looked at the sampling distribution of the sample minimum maximum for a few different kinds of distributions. The four distributions we looked at were normal, uniform, exponential, and beta. We took the information we were given and found both the maximum and minimum E(y) and SE(y). Our take away from this project was that normal and uniform distributions had roughly the same SE(y) for both maximum and minimum because these were symmetric graphs and distributions and therefore had similar SE at both the minimum and max. On the contrary, the exponential and beta distribution had different SE(y) for the minimum and maximum because the graphs were not symmetric and therefore the spread was different at the minimum and maximum. While the key takeaway from just this mini project was that symmetric distributions will have the same SE(Y) for both the min and max, another key takeaway was to introduce us more to these sample distributions, help us get comfortable with what they look like, so that when we begin to look at this distributions later in class with unknown parameters, we understand what they look like and how to work with estimation. This mini project, along with Section 1, and Stat 325, helped build the base for distributions before we moved onto simulations and estimations (Mini Projects 2-5).

In mini project 2, we took a chance at writing a short story to help us understand how the process of estimation works. By writing a story using estimation prompts such as Estimator, Parameter, Random Sample, Bias, etc., we were able to further explore the section 2 topic of estimations and estimators such as the MOM or MLE. Sort of like the first mini project 1, mini project 2 was not about knowing how to do complex stats, but instead about giving us a foundation on estimation to hopefully help us understand what all these terms really mean. With a large chunk of the class using estimations and simulations, it is important that we truly understand each term we are using and how exactly we are using it. Section 3, 4, and 5 all worked in some way with unknown parameters for distributions that we would have to estimate.

In mini project 3, we took a look at simulating confidence intervals to make sure we knew how to use them correctly. We had to select 3 different sample sizes from two different population proportions and record the coverage rate and average width of the 5,000 C.I simulations. Mini project 3 reinforced this idea that we get the best results in Stats, especially when doing estimations, when we have a large sample size that passes the large sample assumption. This helped tied together section 3 of class about using confidence intervals for unknown parameters as we learned that we need to make sure we have a large enough sample size to get good simulations. If we are simulating off a small n, we won't get super accurate and consistent simulations. This idea of having a big enough sample size is key in sections 4 and 5 as both of these sections see that with the more information about an unknown parameter you have, the better results you will get.

In mini project 4, we explored how to build priors based off different levels of information (no information, information on a previous match, information about a claim from an announcer). With each of these priors we updated our priors and built a posterior distribution after observing data from the 2020 French open. This project truly begins to tie in what this class is truly about which is knowing how to work with data when the distribution is not fully known. We will never know the true distribution of points Nadal wins on his own serve against Djokovic. However, we will know how to simulate and estimate what that distribution would look like and that is what we do with Bayesian Analysis. We saw that having an informative prior (prior knowledge on whatever it is you are researching) was better than an non-informative prior. We also found that you do not want a very large prior as this would dominate any data you try to add to the prior. Instead, you want a prior that has some information about what you are researching without having large enough data to where the spread is already super small. This is arguably the most important lesson and section of the class as this is very close to the kind of situations statisticians deal with in the real world. We almost never know the true distribution and instead are trying to find was to estimate or simulate what our parameters and distributions are like,

Finally, in mini project 5, we were tasked with reading an editorial that focused on the usefulness of the p value. In section 5, we worked with a lot of p values and using them to determine whether results were deemed "significant" or "not significant" enough to reject the null. The issue with the p value is a p value of 0.04999999999 will have you reject the null and say your findings were significant, however a p value of 0.05000000000000001 will have you fail to reject the null and say your findings were not significant. Even though the p values are super super close in value, we get two very different results. This mini project helps statisticians realize that we can't just use a p value to observe and analyze our results, We need to look at all factors and information related to our results to determine whether or not they are significant or not. We also should not allow a large p value to be the reason we do not publish our findings. Whether our findings have a large p or not, the information and research is still important and still worth sharing as it can still help others who are interested and working on the same topic. Mini project 5 tied together all of the sections and mini projects together as it tells us that "non significant" results are still results that deserve to be shared. We may not find the best intervals, the best estimators, the best priors, but by sharing our findings we are putting more research out there that is available for others to learn from and build off.

Overall, with each mini project we continued to learn new ideas that connected with each other. We learned a lot about how to work with unknown parameters in different ways and that was a key learning point as the real world is full of unknown parameters.
