[
  {
    "objectID": "posts/Mini-Project-Reflection/index.html",
    "href": "posts/Mini-Project-Reflection/index.html",
    "title": "Mini Project Reflection",
    "section": "",
    "text": "Throughout the semester, we came across 5 mini projects that not only tie together with each other but with the materials we have learned throughout the semester.\nIn mini project 1, we looked at the sampling distribution of the sample minimum maximum for a few different kinds of distributions. The four distributions we looked at were normal, uniform, exponential, and beta. We took the information we were given and found both the maximum and minimum E(y) and SE(y). Our take away from this project was that normal and uniform distributions had roughly the same SE(y) for both maximum and minimum because these were symmetric graphs and distributions and therefore had similar SE at both the minimum and max. On the contrary, the exponential and beta distribution had different SE(y) for the minimum and maximum because the graphs were not symmetric and therefore the spread was different at the minimum and maximum. While the key takeaway from just this mini project was that symmetric distributions will have the same SE(Y) for both the min and max, another key takeaway was to introduce us more to these sample distributions, help us get comfortable with what they look like, so that when we begin to look at this distributions later in class with unknown parameters, we understand what they look like and how to work with estimation. This mini project, along with Section 1, and Stat 325, helped build the base for distributions before we moved onto simulations and estimations (Mini Projects 2-5).\nIn mini project 2, we took a chance at writing a short story to help us understand how the process of estimation works. By writing a story using estimation prompts such as Estimator, Parameter, Random Sample, Bias, etc., we were able to further explore the section 2 topic of estimations and estimators such as the MOM or MLE. Sort of like the first mini project 1, mini project 2 was not about knowing how to do complex stats, but instead about giving us a foundation on estimation to hopefully help us understand what all these terms really mean. With a large chunk of the class using estimations and simulations, it is important that we truly understand each term we are using and how exactly we are using it. Section 3, 4, and 5 all worked in some way with unknown parameters for distributions that we would have to estimate.\nIn mini project 3, we took a look at simulating confidence intervals to make sure we knew how to use them correctly. We had to select 3 different sample sizes from two different population proportions and record the coverage rate and average width of the 5,000 C.I simulations. Mini project 3 reinforced this idea that we get the best results in Stats, especially when doing estimations, when we have a large sample size that passes the large sample assumption. This helped tied together section 3 of class about using confidence intervals for unknown parameters as we learned that we need to make sure we have a large enough sample size to get good simulations. If we are simulating off a small n, we won’t get super accurate and consistent simulations. This idea of having a big enough sample size is key in sections 4 and 5 as both of these sections see that with the more information about an unknown parameter you have, the better results you will get.\nIn mini project 4, we explored how to build priors based off different levels of information (no information, information on a previous match, information about a claim from an announcer). With each of these priors we updated our priors and built a posterior distribution after observing data from the 2020 French open. This project truly begins to tie in what this class is truly about which is knowing how to work with data when the distribution is not fully known. We will never know the true distribution of points Nadal wins on his own serve against Djokovic. However, we will know how to simulate and estimate what that distribution would look like and that is what we do with Bayesian Analysis. We saw that having an informative prior (prior knowledge on whatever it is you are researching) was better than an non-informative prior. We also found that you do not want a very large prior as this would dominate any data you try to add to the prior. Instead, you want a prior that has some information about what you are researching without having large enough data to where the spread is already super small. This is arguably the most important lesson and section of the class as this is very close to the kind of situations statisticians deal with in the real world. We almost never know the true distribution and instead are trying to find was to estimate or simulate what our parameters and distributions are like,\nFinally, in mini project 5, we were tasked with reading an editorial that focused on the usefulness of the p value. In section 5, we worked with a lot of p values and using them to determine whether results were deemed “significant” or “not significant” enough to reject the null. The issue with the p value is a p value of 0.04999999999 will have you reject the null and say your findings were significant, however a p value of 0.05000000000000001 will have you fail to reject the null and say your findings were not significant. Even though the p values are super super close in value, we get two very different results. This mini project helps statisticians realize that we can’t just use a p value to observe and analyze our results, We need to look at all factors and information related to our results to determine whether or not they are significant or not. We also should not allow a large p value to be the reason we do not publish our findings. Whether our findings have a large p or not, the information and research is still important and still worth sharing as it can still help others who are interested and working on the same topic. Mini project 5 tied together all of the sections and mini projects together as it tells us that “non significant” results are still results that deserve to be shared. We may not find the best intervals, the best estimators, the best priors, but by sharing our findings we are putting more research out there that is available for others to learn from and build off.\nOverall, with each mini project we continued to learn new ideas that connected with each other. We learned a lot about how to work with unknown parameters in different ways and that was a key learning point as the real world is full of unknown parameters."
  },
  {
    "objectID": "posts/Mini-Project-4/index.html",
    "href": "posts/Mini-Project-4/index.html",
    "title": "Mini-Project-4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nSince we have no knowledge about Nadal’s playing skill, or even Djokovic, we will assume that there is a 50/50 chance that Nadal will win a point on his own serve against his rival Djokovic. Our non informative alpha and beta will both be equal to 1.\n\n\n\n\n\nSince we know he won 46 our of 66 points, we find that his probability of winning, or as we called it target mean, is 44/66. We also know our target variance is 0.05657^2 since our standard error is 0.05657. We use the following code to calculate the best alpha and beta that give us our target mean and target variance.\n\n\ntarget_mean &lt;- 44/66\nalphas &lt;- seq(0.1, 55, length.out = 500)\nbetas &lt;- ((22/66)*alphas) / (44/66)\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = (alphas*betas) / ((alphas+betas)^2 * (alphas +betas+1)))\n\ntarget_var &lt;- 0.05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\nparam_df\n\n# A tibble: 500 × 4\n   alphas betas   vars dist_to_target\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n 1  0.1   0.05  0.193          0.190 \n 2  0.210 0.105 0.169          0.166 \n 3  0.320 0.160 0.150          0.147 \n 4  0.430 0.215 0.135          0.132 \n 5  0.540 0.270 0.123          0.120 \n 6  0.650 0.325 0.113          0.109 \n 7  0.760 0.380 0.104          0.101 \n 8  0.870 0.435 0.0964         0.0932\n 9  0.980 0.490 0.0900         0.0868\n10  1.09  0.545 0.0843         0.0811\n# ℹ 490 more rows\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.6  22.8 0.00320     0.00000145\n\n\n\n\n\n\nJust like the previous prior, we will use the wins as our target mean, which is 0.75. Since we are almost sure that Nadal wins no less than 70% of his points against Djokovic, we will set our target probability to 0.02. We will use the following code to calculate the best combination of alpha and beta that give us our target mean and probability.\n\n\ntarget_mean &lt;- 0.75\nalphas &lt;- seq(0.1, 400,length.out = 500)\nbetas &lt;- (0.25*alphas) / 0.75\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = (alphas*betas) / ((alphas+betas)^2 * (alphas +betas+1)))\n\ntarget_prob &lt;- 0.02\nprob_less_70 &lt;- pbeta(.70, alphas, betas)\n\ntibble(alphas, betas, prob_less_70) |&gt;\n  mutate(close_to_target = abs(prob_less_70 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_70 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9       0.0200       0.0000117\n\n\n\n\n\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnon_informative_alpha &lt;- 1\nnon_informative_beta &lt;-1\n\ninformative_alpha_1 &lt;- 45.6483\ninformative_beta_1 &lt;-22.82415\n\ninformative_alpha_2 &lt;- 251.7405\ninformative_beta_2 &lt;- 83.91349\n\nnon_informative_prior &lt;- dbeta(ps, non_informative_alpha, non_informative_beta)\n\ninformative_prior_1 &lt;- dbeta(ps, informative_alpha_1,informative_beta_1)\n\ninformative_prior_2 &lt;- dbeta(ps, informative_alpha_2,informative_beta_2)\n\nprior_plot &lt;- tibble(ps, non_informative_prior, informative_prior_1, informative_prior_2 ) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Graph of the Three Priors\")\n\n\n\n\n\n\n\n\n\nAbove, we have graphed all three of our priors.\n\n\n\n\n\n\nNow that we have our priors, we can now take the data from the 2020 French Open to update our prior for the probability that Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open. At the 2020 French Open, when Nadal and Djokovic played in the final, Nadal won 56 points of out 84 served points.\n\n\n\n\nnon_informative_alpha &lt;- 1\nnon_informative_beta &lt;-1\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\nnon_informative_post_alpha &lt;- 56 + non_informative_alpha\nnon_informative_post_beta &lt;- 84 - 56 + non_informative_beta\n\nnon_informative_post_alpha\n\n[1] 57\n\nnon_informative_post_beta\n\n[1] 29\n\n\n\nAs you can see, our post alpha is 57 and our post beta is 29 for our non informative prior\n\n\n\n\n\ninformative_alpha_1 &lt;- 45.6483\ninformative_beta_1 &lt;-22.82415\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\ninformative_post_alpha_1 &lt;- 56 + informative_alpha_1\ninformative_post_beta_1 &lt;- 84 - 56 + informative_beta_1\n\ninformative_post_alpha_1\n\n[1] 101.6483\n\ninformative_post_beta_1\n\n[1] 50.82415\n\n\n\nAs you can see, our post alpha is 101.6483 and our post beta is 50.82415 for our informed prior 1 (based on clay-court match the two played in the previous year).\n\n\n\n\n\ninformative_alpha_2 &lt;- 251.7405\ninformative_beta_2 &lt;- 83.91349\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\ninformative_post_alpha_2 &lt;- 56 + informative_alpha_2\ninformative_post_beta_2 &lt;- 84 - 56 + informative_beta_2\n\ninformative_post_alpha_2\n\n[1] 307.7405\n\ninformative_post_beta_2\n\n[1] 111.9135\n\n\n\nAs you can see, our post alpha is 307.7405 and our post beta is 111.9135 for our informed prior 2 (based on a sports announcers claim).\n\n\n\n\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnon_informative_post &lt;- dbeta(ps, non_informative_post_alpha, non_informative_post_beta)\n\ninformative_post_1 &lt;- dbeta(ps, informative_post_alpha_1,informative_post_beta_1)\n\ninformative_post_2 &lt;- dbeta(ps, informative_post_alpha_2,informative_post_beta_2)\n\npost_plot &lt;- tibble(ps, non_informative_post, informative_post_1, informative_post_2 ) |&gt;\n  pivot_longer(2:4, names_to = \"post_type\", values_to = \"density\")\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = post_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Graph of the Three Posteriors\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have found our posterior distributions and graphed them, we can also compare their posterior means and credible intervals.\n\n\n\n\nnon_informative_post_mean &lt;- 57/(57 + 29)\nnon_informative_post_mean\n\n[1] 0.6627907\n\nqbeta(0.05, 57, 29)\n\n[1] 0.5772453\n\nqbeta(0.95, 57, 29)\n\n[1] 0.7440061\n\n\n\nWe found the non informative posterior mean to be 0.663.We found the 90% credible interval to be (0.5772453, 0.7440061 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.5772453 and 0.7440061.\n\n\n\n\n\ninformative_post_1_mean &lt;- 101.6483/(101.6483 + 50.82415)\ninformative_post_1_mean\n\n[1] 0.6666667\n\nqbeta(0.05, 101.6483, 50.82415)\n\n[1] 0.602825\n\nqbeta(0.95, 101.6483, 50.82415)\n\n[1] 0.7280142\n\n\n\nWe found the informative posterior #1 (based on clay-court match the two played in the previous year) mean for to be 0.666667. We found the 90% credible interval to be (0.602825, 0.7280142 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.602825 and 0.7280142.\n\n\n\n\n\ninformative_post_2_mean &lt;- 307.7405/(307.7405 + 111.9135)\ninformative_post_2_mean\n\n[1] 0.7333196\n\nqbeta(0.05, 307.7405, 111.9135)\n\n[1] 0.6972262\n\nqbeta(0.95, 307.7405, 111.9135)\n\n[1] 0.7681472\n\n\n\nWe found the informative posterior #2 (based on a sports announcers claim) mean for to be 0.7333196. We found the 90% credible interval to be (0.6972262, 0.7681472 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.6972262 and 0.7681472.\n\n\n\n\n\n\nBefore comparing the 3 posteriors to each other, we should compare each posterior to the prior. For our non informative posterior, we see a big change from the prior since our prior was very week. Any chunk of data added to a non informative prior will almost completely control the shape of the posterior distribution. For our first informative posterior (based on clay-court match the two played in the previous year). we see the mean does not change (since the data we collected from 2020 had the same probability as our prior data), however, the variance does decrease since we have increased both alpha and beta. For our second informative posterior (based on sports announcers claim), we see very little change in the mean of the probability, The reason for the slight change was our prior already had a very large alpha and beta. This means any new posterior we want to make will have a very slight effect on the prior.\nNow that we compared each posterior to their own priors, we will prepare the posteriors to the others. First, we noticed that our non informative posterior and 1st informative posterior shared the same exact posterior mean. The only difference between the two is their 90% credible intervals. This is because the 1st informative posterior has a much larger alpha and beta than the non informative posterior, making its 90% credible interval smaller than the non informative posterior. We also can see that the 2nd informative posterior stayed much larger than the other two posterior which made sense to us since the 2nd posterior had a very large alpha and beta to begin with. It also has a smaller credible interval due to the large alpha and beta.\nIf I had to chose which one to use, I would use the 1st informative posterior. Reason #1 would be that the mean of the posterior didn’t change from the prior because our prior data matched the posterior data. The second reason is we can see from the graph the the variance of the posterior is not too large, especially compared to the non-informative prior. Finally, the 2nd posteriors mean was not very close to the data’s mean which is why I would not use the 2nd posterior.\nAs we briefly mentioned above, the second informative posterior had the lowest variance because it had the largest alpha and beta. The non informative posterior has the largest variance since it had to smallest alpha and beta.\n\n\n\n\n\nTo conclude, we can see that the best kind of prior and posterior to use is: A prior that does not overpower the distribution, and a posterior that has a mean that is close to the mean of our data we find. We didn’t get an amazing distribution when we used a non informative prior because the data we collected completely overpowered the prior. While its posterior mean matched our 1st informative posterior, its variance was much larger since it had a much smaller alpha and beta. We didn’t get amazing results with our 2nd informative prior because in order to match the targeted mean and probability, we had to use a very large alpha and beta to begin with. This means when we go to calculate a posterior, the prior will completely overpower the data, therefore not given us the best posterior. The 1st informative prior had a great informative data that, in this case, matched the data collected after the prior. This meant while the mean did not shift, the variance got even smaller which is what we like."
  },
  {
    "objectID": "posts/Mini-Project-4/index.html#priors",
    "href": "posts/Mini-Project-4/index.html#priors",
    "title": "Mini-Project-4",
    "section": "",
    "text": "Since we have no knowledge about Nadal’s playing skill, or even Djokovic, we will assume that there is a 50/50 chance that Nadal will win a point on his own serve against his rival Djokovic. Our non informative alpha and beta will both be equal to 1.\n\n\n\n\n\nSince we know he won 46 our of 66 points, we find that his probability of winning, or as we called it target mean, is 44/66. We also know our target variance is 0.05657^2 since our standard error is 0.05657. We use the following code to calculate the best alpha and beta that give us our target mean and target variance.\n\n\ntarget_mean &lt;- 44/66\nalphas &lt;- seq(0.1, 55, length.out = 500)\nbetas &lt;- ((22/66)*alphas) / (44/66)\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = (alphas*betas) / ((alphas+betas)^2 * (alphas +betas+1)))\n\ntarget_var &lt;- 0.05657^2\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\nparam_df\n\n# A tibble: 500 × 4\n   alphas betas   vars dist_to_target\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n 1  0.1   0.05  0.193          0.190 \n 2  0.210 0.105 0.169          0.166 \n 3  0.320 0.160 0.150          0.147 \n 4  0.430 0.215 0.135          0.132 \n 5  0.540 0.270 0.123          0.120 \n 6  0.650 0.325 0.113          0.109 \n 7  0.760 0.380 0.104          0.101 \n 8  0.870 0.435 0.0964         0.0932\n 9  0.980 0.490 0.0900         0.0868\n10  1.09  0.545 0.0843         0.0811\n# ℹ 490 more rows\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.6  22.8 0.00320     0.00000145\n\n\n\n\n\n\nJust like the previous prior, we will use the wins as our target mean, which is 0.75. Since we are almost sure that Nadal wins no less than 70% of his points against Djokovic, we will set our target probability to 0.02. We will use the following code to calculate the best combination of alpha and beta that give us our target mean and probability.\n\n\ntarget_mean &lt;- 0.75\nalphas &lt;- seq(0.1, 400,length.out = 500)\nbetas &lt;- (0.25*alphas) / 0.75\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = (alphas*betas) / ((alphas+betas)^2 * (alphas +betas+1)))\n\ntarget_prob &lt;- 0.02\nprob_less_70 &lt;- pbeta(.70, alphas, betas)\n\ntibble(alphas, betas, prob_less_70) |&gt;\n  mutate(close_to_target = abs(prob_less_70 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_70 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9       0.0200       0.0000117\n\n\n\n\n\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnon_informative_alpha &lt;- 1\nnon_informative_beta &lt;-1\n\ninformative_alpha_1 &lt;- 45.6483\ninformative_beta_1 &lt;-22.82415\n\ninformative_alpha_2 &lt;- 251.7405\ninformative_beta_2 &lt;- 83.91349\n\nnon_informative_prior &lt;- dbeta(ps, non_informative_alpha, non_informative_beta)\n\ninformative_prior_1 &lt;- dbeta(ps, informative_alpha_1,informative_beta_1)\n\ninformative_prior_2 &lt;- dbeta(ps, informative_alpha_2,informative_beta_2)\n\nprior_plot &lt;- tibble(ps, non_informative_prior, informative_prior_1, informative_prior_2 ) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Graph of the Three Priors\")\n\n\n\n\n\n\n\n\n\nAbove, we have graphed all three of our priors."
  },
  {
    "objectID": "posts/Mini-Project-4/index.html#data-from-the-2020-french-open",
    "href": "posts/Mini-Project-4/index.html#data-from-the-2020-french-open",
    "title": "Mini-Project-4",
    "section": "",
    "text": "Now that we have our priors, we can now take the data from the 2020 French Open to update our prior for the probability that Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open. At the 2020 French Open, when Nadal and Djokovic played in the final, Nadal won 56 points of out 84 served points.\n\n\n\n\nnon_informative_alpha &lt;- 1\nnon_informative_beta &lt;-1\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\nnon_informative_post_alpha &lt;- 56 + non_informative_alpha\nnon_informative_post_beta &lt;- 84 - 56 + non_informative_beta\n\nnon_informative_post_alpha\n\n[1] 57\n\nnon_informative_post_beta\n\n[1] 29\n\n\n\nAs you can see, our post alpha is 57 and our post beta is 29 for our non informative prior\n\n\n\n\n\ninformative_alpha_1 &lt;- 45.6483\ninformative_beta_1 &lt;-22.82415\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\ninformative_post_alpha_1 &lt;- 56 + informative_alpha_1\ninformative_post_beta_1 &lt;- 84 - 56 + informative_beta_1\n\ninformative_post_alpha_1\n\n[1] 101.6483\n\ninformative_post_beta_1\n\n[1] 50.82415\n\n\n\nAs you can see, our post alpha is 101.6483 and our post beta is 50.82415 for our informed prior 1 (based on clay-court match the two played in the previous year).\n\n\n\n\n\ninformative_alpha_2 &lt;- 251.7405\ninformative_beta_2 &lt;- 83.91349\n\n# for alpha posterior, we just add the successes (56) to our prior alpha\n# for beta posterior, we we take the number of trials (84), subtract the successes (56) and then add our prior beta\n\ninformative_post_alpha_2 &lt;- 56 + informative_alpha_2\ninformative_post_beta_2 &lt;- 84 - 56 + informative_beta_2\n\ninformative_post_alpha_2\n\n[1] 307.7405\n\ninformative_post_beta_2\n\n[1] 111.9135\n\n\n\nAs you can see, our post alpha is 307.7405 and our post beta is 111.9135 for our informed prior 2 (based on a sports announcers claim).\n\n\n\n\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnon_informative_post &lt;- dbeta(ps, non_informative_post_alpha, non_informative_post_beta)\n\ninformative_post_1 &lt;- dbeta(ps, informative_post_alpha_1,informative_post_beta_1)\n\ninformative_post_2 &lt;- dbeta(ps, informative_post_alpha_2,informative_post_beta_2)\n\npost_plot &lt;- tibble(ps, non_informative_post, informative_post_1, informative_post_2 ) |&gt;\n  pivot_longer(2:4, names_to = \"post_type\", values_to = \"density\")\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = post_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\", title = \"Graph of the Three Posteriors\")"
  },
  {
    "objectID": "posts/Mini-Project-4/index.html#posterior-means-90-credible-intervals-for-p",
    "href": "posts/Mini-Project-4/index.html#posterior-means-90-credible-intervals-for-p",
    "title": "Mini-Project-4",
    "section": "",
    "text": "Now that we have found our posterior distributions and graphed them, we can also compare their posterior means and credible intervals.\n\n\n\n\nnon_informative_post_mean &lt;- 57/(57 + 29)\nnon_informative_post_mean\n\n[1] 0.6627907\n\nqbeta(0.05, 57, 29)\n\n[1] 0.5772453\n\nqbeta(0.95, 57, 29)\n\n[1] 0.7440061\n\n\n\nWe found the non informative posterior mean to be 0.663.We found the 90% credible interval to be (0.5772453, 0.7440061 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.5772453 and 0.7440061.\n\n\n\n\n\ninformative_post_1_mean &lt;- 101.6483/(101.6483 + 50.82415)\ninformative_post_1_mean\n\n[1] 0.6666667\n\nqbeta(0.05, 101.6483, 50.82415)\n\n[1] 0.602825\n\nqbeta(0.95, 101.6483, 50.82415)\n\n[1] 0.7280142\n\n\n\nWe found the informative posterior #1 (based on clay-court match the two played in the previous year) mean for to be 0.666667. We found the 90% credible interval to be (0.602825, 0.7280142 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.602825 and 0.7280142.\n\n\n\n\n\ninformative_post_2_mean &lt;- 307.7405/(307.7405 + 111.9135)\ninformative_post_2_mean\n\n[1] 0.7333196\n\nqbeta(0.05, 307.7405, 111.9135)\n\n[1] 0.6972262\n\nqbeta(0.95, 307.7405, 111.9135)\n\n[1] 0.7681472\n\n\n\nWe found the informative posterior #2 (based on a sports announcers claim) mean for to be 0.7333196. We found the 90% credible interval to be (0.6972262, 0.7681472 ) According to our model, there is a 90% probability that the proportion of points that Nadal wins on his own serve against his primary rival, Novak Djokovic, at the French Open is between 0.6972262 and 0.7681472."
  },
  {
    "objectID": "posts/Mini-Project-4/index.html#comparing-the-3-posteriors",
    "href": "posts/Mini-Project-4/index.html#comparing-the-3-posteriors",
    "title": "Mini-Project-4",
    "section": "",
    "text": "Before comparing the 3 posteriors to each other, we should compare each posterior to the prior. For our non informative posterior, we see a big change from the prior since our prior was very week. Any chunk of data added to a non informative prior will almost completely control the shape of the posterior distribution. For our first informative posterior (based on clay-court match the two played in the previous year). we see the mean does not change (since the data we collected from 2020 had the same probability as our prior data), however, the variance does decrease since we have increased both alpha and beta. For our second informative posterior (based on sports announcers claim), we see very little change in the mean of the probability, The reason for the slight change was our prior already had a very large alpha and beta. This means any new posterior we want to make will have a very slight effect on the prior.\nNow that we compared each posterior to their own priors, we will prepare the posteriors to the others. First, we noticed that our non informative posterior and 1st informative posterior shared the same exact posterior mean. The only difference between the two is their 90% credible intervals. This is because the 1st informative posterior has a much larger alpha and beta than the non informative posterior, making its 90% credible interval smaller than the non informative posterior. We also can see that the 2nd informative posterior stayed much larger than the other two posterior which made sense to us since the 2nd posterior had a very large alpha and beta to begin with. It also has a smaller credible interval due to the large alpha and beta.\nIf I had to chose which one to use, I would use the 1st informative posterior. Reason #1 would be that the mean of the posterior didn’t change from the prior because our prior data matched the posterior data. The second reason is we can see from the graph the the variance of the posterior is not too large, especially compared to the non-informative prior. Finally, the 2nd posteriors mean was not very close to the data’s mean which is why I would not use the 2nd posterior.\nAs we briefly mentioned above, the second informative posterior had the lowest variance because it had the largest alpha and beta. The non informative posterior has the largest variance since it had to smallest alpha and beta."
  },
  {
    "objectID": "posts/Mini-Project-4/index.html#conclusion",
    "href": "posts/Mini-Project-4/index.html#conclusion",
    "title": "Mini-Project-4",
    "section": "",
    "text": "To conclude, we can see that the best kind of prior and posterior to use is: A prior that does not overpower the distribution, and a posterior that has a mean that is close to the mean of our data we find. We didn’t get an amazing distribution when we used a non informative prior because the data we collected completely overpowered the prior. While its posterior mean matched our 1st informative posterior, its variance was much larger since it had a much smaller alpha and beta. We didn’t get amazing results with our 2nd informative prior because in order to match the targeted mean and probability, we had to use a very large alpha and beta to begin with. This means when we go to calculate a posterior, the prior will completely overpower the data, therefore not given us the best posterior. The 1st informative prior had a great informative data that, in this case, matched the data collected after the prior. This meant while the mean did not shift, the variance got even smaller which is what we like."
  },
  {
    "objectID": "posts/Mini-Project-2/index.html",
    "href": "posts/Mini-Project-2/index.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "“All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project”"
  },
  {
    "objectID": "posts/Mini-Project-2/index.html#while-in-the-off-season-coach-puck-of-the-st.-lawrence-football-team-came-up-to-me-with-a-predicament.-he-wanted-me-to-try-to-find-how-many-games-we-could-expect-to-win-this-upcoming-season.-being-someone-who-loves-football-and-statistics-this-was-a-no-brainer-for-me.-to-find-out-this-answer-i-figured-i-could-use-a-binomial-distribution-to-figure-out-whether-a-team-would-either-win-or-lose-the-game.-our-random-variable-x-would-be-the-number-of-games-the-st.-lawrence-football-team-will-win-in-a-10-game-season-our-n-will-be-10-for-the-number-of-trials-or-games-in-a-season-but-our-p-probability-of-the-team-winning-a-single-game-is-an-unknown-parameter.-this-is-an-issue-but-we-can-solve-it-by-using-a-maximum-likelihood-estimation-to-find-an-estimator-for-our-p.-first-we-will-take-a-random-sample-of-3-10-game-seasons-from-the-last-15-10-game-seasons-played-by-the-saints-with-x-representing-the-number-of-games-won-in-that-season.-this-sample-will-be-independent-and-identically-distributed.-next-we-will-find-the-likelihood-function-of-a-binomial-distribution-x1-x2-x3-i.i.d-binomn-10-p.-we-will-use-the-maximum-likelihood-estimation-mle-to-derive-an-estimator-for-our-unknown-p.-when-we-do-all-the-steps-associated-with-mle-we-will-get-the-mle-for-our-p.-when-we-plug-in-our-random-sample-we-will-now-how-an-estimate-for-our-p.-while-it-would-be-very-easy-to-say-yeah-we-found-our-p-and-can-now-estimate-how-many-games-we-will-win-this-year-what-we-dont-know-is-if-this-is-the-best-estimator.-to-help-with-this-we-can-use-a-different-estimator-method-and-use-the-method-of-moments-estimators-mom.-after-doing-the-steps-associated-with-the-mom-we-will-now-have-two-different-estimators-the-pmle-and-pmom.-now-the-question-is-which-one-is-better-to-discover-this-we-can-find-each-estimators-bias-variance-and-consistency.-bias-will-tell-us-how-far-off-the-expectation-of-our-estimator-is-for-p-from-the-actual-p.-generally-you-want-an-estimator-that-is-unbiased.-next-we-can-look-at-the-variance-of-our-estimators.-if-both-of-our-estimators-are-unbiased-then-we-can-use-variance-to-see-which-estimator-is-more-efficient-than-the-other.-generally-speaking-we-want-an-estimator-that-is-unbiased-and-has-a-high-efficiency-compared-to-the-other-estimator.-finally-we-can-look-to-see-whether-the-estimators-are-consistent.-to-be-consistent-the-estimator-must-be-asymptotically-unbiased-and-its-variance-must-converge-to-0.-after-all-these-steps-we-will-find-a-very-strong-estimator-to-answer-the-question-of-how-many-games-the-st.-lawrence-football-team-will-win-this-year.",
    "href": "posts/Mini-Project-2/index.html#while-in-the-off-season-coach-puck-of-the-st.-lawrence-football-team-came-up-to-me-with-a-predicament.-he-wanted-me-to-try-to-find-how-many-games-we-could-expect-to-win-this-upcoming-season.-being-someone-who-loves-football-and-statistics-this-was-a-no-brainer-for-me.-to-find-out-this-answer-i-figured-i-could-use-a-binomial-distribution-to-figure-out-whether-a-team-would-either-win-or-lose-the-game.-our-random-variable-x-would-be-the-number-of-games-the-st.-lawrence-football-team-will-win-in-a-10-game-season-our-n-will-be-10-for-the-number-of-trials-or-games-in-a-season-but-our-p-probability-of-the-team-winning-a-single-game-is-an-unknown-parameter.-this-is-an-issue-but-we-can-solve-it-by-using-a-maximum-likelihood-estimation-to-find-an-estimator-for-our-p.-first-we-will-take-a-random-sample-of-3-10-game-seasons-from-the-last-15-10-game-seasons-played-by-the-saints-with-x-representing-the-number-of-games-won-in-that-season.-this-sample-will-be-independent-and-identically-distributed.-next-we-will-find-the-likelihood-function-of-a-binomial-distribution-x1-x2-x3-i.i.d-binomn-10-p.-we-will-use-the-maximum-likelihood-estimation-mle-to-derive-an-estimator-for-our-unknown-p.-when-we-do-all-the-steps-associated-with-mle-we-will-get-the-mle-for-our-p.-when-we-plug-in-our-random-sample-we-will-now-how-an-estimate-for-our-p.-while-it-would-be-very-easy-to-say-yeah-we-found-our-p-and-can-now-estimate-how-many-games-we-will-win-this-year-what-we-dont-know-is-if-this-is-the-best-estimator.-to-help-with-this-we-can-use-a-different-estimator-method-and-use-the-method-of-moments-estimators-mom.-after-doing-the-steps-associated-with-the-mom-we-will-now-have-two-different-estimators-the-pmle-and-pmom.-now-the-question-is-which-one-is-better-to-discover-this-we-can-find-each-estimators-bias-variance-and-consistency.-bias-will-tell-us-how-far-off-the-expectation-of-our-estimator-is-for-p-from-the-actual-p.-generally-you-want-an-estimator-that-is-unbiased.-next-we-can-look-at-the-variance-of-our-estimators.-if-both-of-our-estimators-are-unbiased-then-we-can-use-variance-to-see-which-estimator-is-more-efficient-than-the-other.-generally-speaking-we-want-an-estimator-that-is-unbiased-and-has-a-high-efficiency-compared-to-the-other-estimator.-finally-we-can-look-to-see-whether-the-estimators-are-consistent.-to-be-consistent-the-estimator-must-be-asymptotically-unbiased-and-its-variance-must-converge-to-0.-after-all-these-steps-we-will-find-a-very-strong-estimator-to-answer-the-question-of-how-many-games-the-st.-lawrence-football-team-will-win-this-year.",
    "title": "Mini Project 2",
    "section": "While in the off-season, coach Puck of the St. Lawrence Football team came up to me with a predicament. He wanted me to try to find how many games we could expect to win this upcoming season. Being someone who loves football and statistics, this was a no brainer for me. To find out this answer, I figured I could use a binomial distribution to figure out whether a team would either win or lose the game. Our random variable, x, would be the number of games the St. Lawrence Football Team will win in a 10-game season, our n will be 10 (for the number of trials or games in a season), but our p (probability of the team winning a single game) is an unknown parameter. This is an issue, but we can solve it by using a maximum likelihood estimation to find an estimator for our p. First, we will take a random sample of 3 10 game seasons from the last 15 10 game seasons played by the Saints, with x representing the number of games won in that season. This sample will be independent and identically distributed. Next, we will find the likelihood function of a binomial distribution X1, X2, X3 i.i.d ~ Binom(n = 10, p). We will use the maximum likelihood estimation (MLE) to derive an estimator for our unknown p. When we do all the steps associated with MLE, we will get the MLE for our P. When we plug in our random sample, we will now how an estimate for our p. While it would be very easy to say “Yeah! We found our p and can now estimate how many games we will win this year”, what we don’t know is if this is the best estimator. To help with this, we can use a different estimator method and use the Method of Moments Estimators (MOM). After doing the steps associated with the MOM, we will now have two different estimators, the Pmle, and Pmom. Now the question is which one is better? To discover this, we can find each estimator’s bias, variance, and consistency. Bias will tell us how far off the expectation of our estimator is for p from the actual p. Generally, you want an estimator that is unbiased. Next, we can look at the variance of our estimators. If both of our estimators are unbiased, then we can use variance to see which estimator is more efficient than the other. Generally speaking, we want an estimator that is unbiased and has a high efficiency compared to the other estimator. Finally, we can look to see whether the estimators are consistent. To be consistent, the estimator must be asymptotically unbiased, and its variance must converge to 0. After all these steps, we will find a very strong estimator to answer the question of how many games the St. Lawrence Football team will win this year.",
    "text": "While in the off-season, coach Puck of the St. Lawrence Football team came up to me with a predicament. He wanted me to try to find how many games we could expect to win this upcoming season. Being someone who loves football and statistics, this was a no brainer for me. To find out this answer, I figured I could use a binomial distribution to figure out whether a team would either win or lose the game. Our random variable, x, would be the number of games the St. Lawrence Football Team will win in a 10-game season, our n will be 10 (for the number of trials or games in a season), but our p (probability of the team winning a single game) is an unknown parameter. This is an issue, but we can solve it by using a maximum likelihood estimation to find an estimator for our p. First, we will take a random sample of 3 10 game seasons from the last 15 10 game seasons played by the Saints, with x representing the number of games won in that season. This sample will be independent and identically distributed. Next, we will find the likelihood function of a binomial distribution X1, X2, X3 i.i.d ~ Binom(n = 10, p). We will use the maximum likelihood estimation (MLE) to derive an estimator for our unknown p. When we do all the steps associated with MLE, we will get the MLE for our P. When we plug in our random sample, we will now how an estimate for our p. While it would be very easy to say “Yeah! We found our p and can now estimate how many games we will win this year”, what we don’t know is if this is the best estimator. To help with this, we can use a different estimator method and use the Method of Moments Estimators (MOM). After doing the steps associated with the MOM, we will now have two different estimators, the Pmle, and Pmom. Now the question is which one is better? To discover this, we can find each estimator’s bias, variance, and consistency. Bias will tell us how far off the expectation of our estimator is for p from the actual p. Generally, you want an estimator that is unbiased. Next, we can look at the variance of our estimators. If both of our estimators are unbiased, then we can use variance to see which estimator is more efficient than the other. Generally speaking, we want an estimator that is unbiased and has a high efficiency compared to the other estimator. Finally, we can look to see whether the estimators are consistent. To be consistent, the estimator must be asymptotically unbiased, and its variance must converge to 0. After all these steps, we will find a very strong estimator to answer the question of how many games the St. Lawrence Football team will win this year."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat326_Final_Portfolio",
    "section": "",
    "text": "Mini Project Reflection\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 5\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 3\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 1\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\nClayton Fogler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Final Portfolio that includes all of my mini projects that I have completed this year, including a reflection on the projects as a whole"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html",
    "href": "posts/Mini-Project-1/index.html",
    "title": "Mini-Project-1",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## create population graphs\n\nnorm_df &lt;- tibble(x = seq(3, 17, length.out = 1000),\n                  dens = dnorm(x, mean = 10, sd = 2),\n                  pop = \"normal(10, 4)\")\nunif_df &lt;- tibble(x = seq(7, 13, length.out = 1000),\n                  dens = dunif(x, 7, 13),\n                  pop = \"uniform(7, 13)\")\nexp_df &lt;- tibble(x = seq(0, 10, length.out = 1000),\n                 dens = dexp(x, 0.5),\n                 pop = \"exp(0.5)\")\nbeta_df &lt;- tibble(x = seq(0, 1, length.out = 1000),\n                  dens = dbeta(x, 8, 2),\n                  pop = \"beta(8, 2)\")\n\npop_plot &lt;- bind_rows(norm_df, unif_df, exp_df, beta_df) |&gt;\n  mutate(pop = fct_relevel(pop, c(\"normal(10, 4)\", \"uniform(7, 13)\",\n                                  \"exp(0.5)\", \"beta(8, 2)\")))\n\nggplot(data = pop_plot, aes(x = x, y = dens)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~ pop, nrow = 1, scales = \"free\") +\n  labs(title = \"Population Distributions for Each Simulation Setting\")"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html#normal-distribution-mu-10-sigma2",
    "href": "posts/Mini-Project-1/index.html#normal-distribution-mu-10-sigma2",
    "title": "Mini-Project-1",
    "section": "Normal Distribution, mu = 10, sigma^2",
    "text": "Normal Distribution, mu = 10, sigma^2\n\nSample Minimum\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_min &lt;- function(mu, sigma, n) {\nsingle_sample &lt;- rnorm(n, mu, sigma)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 8.549541\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  9.19\n 2  6.66\n 3  7.33\n 4  6.56\n 5  6.71\n 6  7.79\n 7  7.47\n 8  7.31\n 9  7.03\n10  9.51\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          7.69          1.78         1.33\n\n\n\n\nSample Maximum\n\nn &lt;- 5 # sample size\nmu &lt;- 10 # population mean\nsigma &lt;- 2 # population standard deviation\ngenerate_samp_max &lt;- function(mu, sigma, n) {\nsingle_sample &lt;- rnorm(n, mu, sigma)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 12.84016\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  15.4\n 2  12.6\n 3  13.8\n 4  12.4\n 5  12.6\n 6  10.4\n 7  13.6\n 8  14.2\n 9  13.4\n10  12.6\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.3          1.74         1.32"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html#uniform-distribution-theta1-7-theta2-13",
    "href": "posts/Mini-Project-1/index.html#uniform-distribution-theta1-7-theta2-13",
    "title": "Mini-Project-1",
    "section": "Uniform Distribution: theta1 = 7, theta2 = 13",
    "text": "Uniform Distribution: theta1 = 7, theta2 = 13\n\nSample Minimum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2)/2 # population mean\nsigma &lt;- ((theta2 - theta1) ^ 2) / 12 # population standard deviation\ngenerate_samp_min &lt;- function(theta1 = theta1, theta2 = theta2, n = n) {\nsingle_sample &lt;- runif(n, theta1, theta2)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(theta1 = theta1, theta2 = theta2, n = n)\n\n[1] 7.863119\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(theta1 = theta1, theta2 = theta2, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.24\n 2  7.51\n 3  7.06\n 4  8.10\n 5 11.0 \n 6  8.05\n 7  9.16\n 8  8.08\n 9  7.83\n10  7.11\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          7.98         0.698        0.835\n\n\n\n\nSample Maximum\n\nn &lt;- 5 # sample size\ntheta1 &lt;- 7\ntheta2 &lt;- 13\nmu &lt;- (theta1 + theta2)/2 # population mean\nsigma &lt;- ((theta2 - theta1) ^ 2) / 12 # population standard deviation\ngenerate_samp_max &lt;- function(theta1 = theta1, theta2 = theta2, n = n) {\nsingle_sample &lt;- runif(n, theta1, theta2)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(theta1 = theta1, theta2 = theta2, n = n)\n\n[1] 12.7376\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(theta1 = theta1, theta2 = theta2, n = n))\n## print some of the 5000 mins\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  13.0\n 2  12.0\n 3  12.7\n 4  10.0\n 5  12.1\n 6  11.3\n 7  12.6\n 8  12.5\n 9  12.5\n10  11.5\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          12.0         0.690        0.831"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html#exponential-distribution-lambda-0.5",
    "href": "posts/Mini-Project-1/index.html#exponential-distribution-lambda-0.5",
    "title": "Mini-Project-1",
    "section": "Exponential Distribution, lambda = 0.5",
    "text": "Exponential Distribution, lambda = 0.5\n\nSample Minimum\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda # population mean\nsigma &lt;- sqrt(1/lambda ^ 2) # population standard deviation\ngenerate_samp_min &lt;- function(lambda = lambda, n = n) {\nsingle_sample &lt;- rexp(n, lambda)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(lambda = lambda, n = n)\n\n[1] 0.8510727\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(lambda = lambda, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n     mins\n    &lt;dbl&gt;\n 1 0.512 \n 2 0.0375\n 3 0.123 \n 4 0.0912\n 5 0.108 \n 6 0.0504\n 7 0.127 \n 8 0.153 \n 9 0.102 \n10 0.504 \n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.409         0.167        0.408\n\n\n\n\nSample Maximum\n\nn &lt;- 5 # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda # population mean\nsigma &lt;- sqrt(1/lambda ^ 2) # population standard deviation\ngenerate_samp_max &lt;- function(lambda = lambda, n = n) {\nsingle_sample &lt;- rexp(n, lambda)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(lambda = lambda, n = n)\n\n[1] 2.51232\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(lambda = lambda, n = n))\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  4.23\n 2  8.00\n 3  4.11\n 4  2.37\n 5  4.05\n 6  5.90\n 7  5.28\n 8  5.89\n 9  2.44\n10  8.22\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          4.60          5.80         2.41"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html#beta-distribution-alpha-8-beta-2",
    "href": "posts/Mini-Project-1/index.html#beta-distribution-alpha-8-beta-2",
    "title": "Mini-Project-1",
    "section": "Beta Distribution, alpha = 8, beta = 2",
    "text": "Beta Distribution, alpha = 8, beta = 2\n\nSample Minimum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta) # population mean\nsigma &lt;- (alpha * beta) / ((alpha + beta) ^ 2 * (alpha + beta + 1)) # population standard deviation\ngenerate_samp_min &lt;- function(alpha, beta, n) {\nsingle_sample &lt;- rbeta(n, alpha, beta)\nsample_min &lt;- min(single_sample)\nreturn(sample_min)\n}\n## test function once:\ngenerate_samp_min(alpha = alpha, beta = beta, n = n)\n\n[1] 0.6856087\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(alpha = alpha, beta = beta, n = n))\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.712\n 2 0.517\n 3 0.587\n 4 0.734\n 5 0.731\n 6 0.613\n 7 0.640\n 8 0.598\n 9 0.709\n10 0.771\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Mins\",\ntitle = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\n\n\nmins_df |&gt;\nsummarise(min_samp_dist = mean(mins),\nvar_samp_dist = var(mins),\nsd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.646        0.0112        0.106\n\n\n\n\nSample Maximum\n\nn &lt;- 5 # sample size\nalpha &lt;- 8\nbeta &lt;- 2\nmu &lt;- alpha / (alpha + beta) # population mean\nsigma &lt;- (alpha * beta) / ((alpha + beta) ^ 2 * (alpha + beta + 1)) # population standard deviation\ngenerate_samp_max &lt;- function(alpha, beta, n) {\nsingle_sample &lt;- rbeta(n, alpha, beta)\nsample_max &lt;- max(single_sample)\nreturn(sample_max)\n}\n## test function once:\ngenerate_samp_max(alpha = alpha, beta = beta, n = n)\n\n[1] 0.9632449\n\nnsim &lt;- 5000 # number of simulations\n## code to map through the function.\n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(alpha = alpha, beta = beta, n = n))\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1 0.959\n 2 0.914\n 3 0.864\n 4 0.983\n 5 0.933\n 6 0.939\n 7 0.972\n 8 0.921\n 9 0.917\n10 0.880\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\ngeom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\ntheme_minimal() +\nlabs(x = \"Observed Sample Maxs\",\ntitle = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\n\n\nmaxs_df |&gt;\nsummarise(max_samp_dist = mean(maxs),\nvar_samp_dist = var(maxs),\nsd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.923       0.00207       0.0455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.66\n8.01\n0.40\n0.65\n\n\n\\(\\text{E}(Y_{max})\\)\n12.32\n12.01\n4.56\n0.92\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.36\n0.84\n0.40\n0.11\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.34\n0.85\n2.41\n0.46"
  },
  {
    "objectID": "posts/Mini-Project-1/index.html#follow-up-questions",
    "href": "posts/Mini-Project-1/index.html#follow-up-questions",
    "title": "Mini-Project-1",
    "section": "Follow Up Questions",
    "text": "Follow Up Questions\n\nQuestion 1: Briefly summarise how SE(Ymin) and SE(Ymax) compare for each of the above population models. Can you propose a general rule or result for how SE(Ymin) and SE(Ymax) compare for a given population?\n\nFor both Normal and uniform distributions, the SE for both Ymin and Ymax are very similar. This is no suprise because both of the distributions are “even”, meaning if you draw a line down the middle, they would be identical on both sides. Because of this, you would expect the SE to be the same on both ends. A general rule for these two would be you only need to find the SE for one of the two, and then once you find it you know the SE for both.\nNow, for Exponential and Beta distributions, the SE(Ymin) was different than the SE(Ymax). Why is this the case? Well, for exponential, the SE(Ymin) was small because when you look at the graph, it is not spread out. On the other hand, SE(Ymax) is much bigger. This makes sense because on the max side, y is approaching 0, which gives the max a wide range. This is a similar case for the beta distribution, however, the SE(Ymax) is small while the SE(Ymin) is large. The only general rule I could think of is the end that approaches 0 will have a larger SE than the end that does not approach 0.\n\n\n\nQuestion 2: Choose either the third (Exponetial) or fourth (beta) population model from the table above. For that population model, find the pdf of Ymin and Ymax, and, for each of those random variables, sketch the pdfs and use integration to calculate the expected value and standard error. What do you notice about how your answers compare to the simulated answers? Some code it given below to help you plot the pdfs in R:\n\nUsing the exponential population model, To find the pdf for Ymin and Ymax, we start with finding the cdf. We can do this by finding the integral from o to x of 0.5e ^ (-0.5y) dy which is equal to 1 - e ^ (-0.5y). With this, we can now find:\n\nfmin(y) = 5(1 - (1 - e ^ (-0.5x))) ^ 4 * 0.5e ^ (-0.5x)\nfmax(y) = 5(1 - e ^ (-0.5x)) ^ 4 * 0.5e ^ (-0.5x)\n\n\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 3, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * (1 - (1 - exp(1) ^ (-0.5 * x))) ^ n - 1 * 0.5 * exp(1) ^ (-0.5 * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(4, 7, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;-  n * (1 - exp(1) ^ (-0.5 * x))  ^ n - 1 * 0.5 * exp(1) ^ (-0.5 * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTheoretical: E(Ymin) and SE(Ymin)\n\nTo find the E(Ymin), we calculate the integral from 0 to infinity of x * 5(1 - (1 - e ^ (-0.5x))) ^ 4 * 0.5e ^ (-0.5x). We get 0.4. This is very very close to our simulated E(Ymin). For SE(Ymin), we must calculate the Var(Ymin). We calculate the integral from 0 to infinity of x^2 * 5(1 - (1 - e ^ (-0.5y))) ^ 4 * 0.5e ^ (-0.5y). We get 0.32. We then use this to do 0.32 - (0.4)^2 = 0.16. When we take the square root of 0.16, we get the SE(Ymin) = 0.4. This is very similar to our simulated SE(Ymin).\n\nTheoretical: E(Ymax) and SE(Ymax)\n\nTo find the E(Ymax), we calculate the integral from 0 to infinity of x * 5(1 - e ^ (-0.5x)) ^ 4 * 0.5e ^ (-0.5x). We get 4.5666. This is very very close to our simulated E(Ymax). For SE(Ymax), we must calculate the Var(Ymax). We calculate the integral from 0 to infinity of x^2 * 5(1 - e ^ (-0.5x)) ^ 4 * 0.5e ^ (-0.5x). We get 26.7088. We then use this to do 26.7088 - (4.5666)^2 = 5.855. When we take the square root of 5.855, we get the SE(Ymax) = 2.4197. This is very similar to our simulated SE(Ymax)."
  },
  {
    "objectID": "posts/Mini-Project-3/index.html",
    "href": "posts/Mini-Project-3/index.html",
    "title": "Mini-Project-3",
    "section": "",
    "text": "“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\n\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\n\n\n\n\nn = 5, n = 20, n = 50\np = 0.46, p = 0.75\n\nNote: These 3 different sample sizes and 2 different population proportions meets the requirments in which at least two of the six settings violate the large sample assumption for the asymptotic confidence interval while two of the six settings hold the assumption.\n\n\n\n\n\n\n\nThis assumption holds since np &gt; 10 and n(1-p) &gt; 10\n\n\n50*0.46\n\n[1] 23\n\n50*(1-0.46)\n\n[1] 27\n\n\n\nConstructing 5,000 sample proportions and 5000 associated C.I.\n\n\ngenerate_onesamp_cis &lt;- function(n, p, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rbinom(1, n, p)\n  \n  ## compute the bounds of the C.I.\n  point_est &lt;- x/n\n  lb &lt;- point_est - qnorm(1 - alpha / 2) * sqrt(point_est * (1 - point_est)/ n)\n  ub &lt;- point_est + qnorm(1 - alpha / 2) * sqrt(point_est * (1 - point_est)/ n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(point_est, lb, ub)\n  \n  return(out_df)\n}\n\n\n## define parameters to use in our function\nn &lt;- 50 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.48 0.364 0.596\n 2      0.5  0.384 0.616\n 3      0.56 0.445 0.675\n 4      0.3  0.193 0.407\n 5      0.44 0.325 0.555\n 6      0.4  0.286 0.514\n 7      0.6  0.486 0.714\n 8      0.54 0.424 0.656\n 9      0.44 0.325 0.555\n10      0.42 0.305 0.535\n# ℹ 4,990 more rows\n\n\n\n\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.230         0.885\n\n\n\n\n\n\n\nThis assumption holds since np &gt; 10 and n(1-p) &gt; 10\n\n50*0.75\n\n[1] 37.5\n\n50*(1-0.75)\n\n[1] 12.5\n\n\n\n## define parameters to use in our function\nn &lt;- 50 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.72 0.616 0.824\n 2      0.74 0.638 0.842\n 3      0.72 0.616 0.824\n 4      0.76 0.661 0.859\n 5      0.7  0.593 0.807\n 6      0.76 0.661 0.859\n 7      0.68 0.571 0.789\n 8      0.76 0.661 0.859\n 9      0.8  0.707 0.893\n10      0.76 0.661 0.859\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.199         0.878\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are both not true\n\n5*0.46\n\n[1] 2.3\n\n5*(1-0.46)\n\n[1] 2.7\n\n\n\n## define parameters to use in our function\nn &lt;- 5 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est      lb    ub\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       0.4  0.0396 0.760\n 2       0.4  0.0396 0.760\n 3       0.6  0.240  0.960\n 4       0.2 -0.0942 0.494\n 5       0.2 -0.0942 0.494\n 6       0.4  0.0396 0.760\n 7       0.2 -0.0942 0.494\n 8       0.4  0.0396 0.760\n 9       0.8  0.506  1.09 \n10       0.6  0.240  0.960\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.630         0.807\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are both not true\n\n5*0.75\n\n[1] 3.75\n\n5*(1-0.75)\n\n[1] 1.25\n\n\n\n## define parameters to use in our function\nn &lt;- 5 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est     lb    ub\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1       1   1      1    \n 2       0.8 0.506  1.09 \n 3       0.8 0.506  1.09 \n 4       0.6 0.240  0.960\n 5       0.8 0.506  1.09 \n 6       0.8 0.506  1.09 \n 7       1   1      1    \n 8       0.8 0.506  1.09 \n 9       0.8 0.506  1.09 \n10       0.4 0.0396 0.760\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.494         0.746\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are not both true\n\n20*0.46\n\n[1] 9.2\n\n20*(1-0.46)\n\n[1] 10.8\n\n\n\n## define parameters to use in our function\nn &lt;- 20 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.5  0.316 0.684\n 2      0.55 0.367 0.733\n 3      0.35 0.175 0.525\n 4      0.65 0.475 0.825\n 5      0.35 0.175 0.525\n 6      0.55 0.367 0.733\n 7      0.6  0.420 0.780\n 8      0.45 0.267 0.633\n 9      0.5  0.316 0.684\n10      0.35 0.175 0.525\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.357          0.88\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are not both true.\n\n20*0.75\n\n[1] 15\n\n20*(1-0.75)\n\n[1] 5\n\n\n\n## define parameters to use in our function\nn &lt;- 20 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.8  0.653 0.947\n 2      0.8  0.653 0.947\n 3      0.8  0.653 0.947\n 4      0.8  0.653 0.947\n 5      0.85 0.719 0.981\n 6      0.65 0.475 0.825\n 7      0.7  0.531 0.869\n 8      0.8  0.653 0.947\n 9      0.55 0.367 0.733\n10      0.7  0.531 0.869\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.308         0.876\n\n\n\n\n\n\n\n\nTable of Results\n\n\n\n\nn = 5\nn = 20\nn = 50\n\n\n\n\n\\(p = 0.46\\)\nCoverage Rate\n0.8048\n0.8812\n0.8812\n\n\n\\(p = 0.75\\)\nCoverage Rate\n0.7474\n0.8626\n0.8774\n\n\n\n\n\n\n\n\n\n\\(p = 0.46\\)\nAverage Width\n0.6275\n0.3571\n0.2295\n\n\n\\(p = 0.75\\)\nAverage Width\n0.4950\n0.3065\n0.1987\n\n\n\n\n\n\n\nOur findings can be broken into three different sample size: n = 5, n = 20, and n = 50.\nFirst, n = 5. Whether p = 0.46 or p = 0.75, they both did not hold the “large sample assumption”. After the 5000 simulations, when p = 0.46, we got an average width of 0.6275 and coverage rate of 0.8048. We are not surprised that the average width is big because our n is so small. Our converge rate is roughly about 0.80, which is not what we wanted since we are doing a 90% C.I.This can be explained because with such a small n, our phats are ending up outside our confidence interval. When p = 0.75, we got an average width of 0.4950 and a coverage rate of 0.7474. Again, we are not surprised by the big interval and smaller than expected coverage rate due to the fact we have a small n and did not hold the “large sample assumption”.\nWhen n = 20, we still saw that it didn’t matter what our p was. Whether it was 0.46 or 0.75, the “large sample assumption” still does not hold. After the 5000 simulations, when p = 0.46, the average width was 0.3571 and coverage rate of 0.8812. We notice that our average width of the confidence interval has decreased from n = 5, which makes sense as a larger sample size should give us a smaller confidence interval. We also notice that our coverage rate was closer to the 0.9 we are looking for. This coverage rate was much closer due to the fact that one of the two test did pass, with one of the test being very close to passing. When p = 0.75, we get an average width of 0.3065 and a coverage rate of 0.8626. Again, like the p = 0.46, we noticed improved numbers as our confidence interval was smaller and our coverage rate was much closer to 0.9 then when n = 5.\nFinally, when n = 50, we saw that when our p = 0.46 or p = 0.75, the “large sample assumption” passed. When p = 0.46, we got an average width of 0.2295 and a coverage rate of 0.8812. We notice again as we increased n, we obtain an even smaller average width and a coverage rate of almost 0.90 This is great as this is what we expect when trying to take a 90% C.I. We get very similar results when p = 0.75 as average width was 0.1987 (the smallest out of all 6 examples) and a coverage rate of 0.8774. This was the smallest average width, which was not suprising as n = 50. This was also again a very close coverage rate to 0.90 as both “large sample assumptions passed”.\nOverall, the biggest take away from our simulations is we get the smallest average width of an interval and best coverage rate when we have a large n and pass the “large sample assumption”."
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#step-1",
    "href": "posts/Mini-Project-3/index.html#step-1",
    "title": "Mini-Project-3",
    "section": "",
    "text": "n = 5, n = 20, n = 50\np = 0.46, p = 0.75\n\nNote: These 3 different sample sizes and 2 different population proportions meets the requirments in which at least two of the six settings violate the large sample assumption for the asymptotic confidence interval while two of the six settings hold the assumption."
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#step-2-starting-with-one-of-the-setting-that-holds-the-large-sample-assumption-construct-at-least-5000-sample-proportions-and-5000-associated-confidence-intervals",
    "href": "posts/Mini-Project-3/index.html#step-2-starting-with-one-of-the-setting-that-holds-the-large-sample-assumption-construct-at-least-5000-sample-proportions-and-5000-associated-confidence-intervals",
    "title": "Mini-Project-3",
    "section": "",
    "text": "This assumption holds since np &gt; 10 and n(1-p) &gt; 10\n\n\n50*0.46\n\n[1] 23\n\n50*(1-0.46)\n\n[1] 27\n\n\n\nConstructing 5,000 sample proportions and 5000 associated C.I.\n\n\ngenerate_onesamp_cis &lt;- function(n, p, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rbinom(1, n, p)\n  \n  ## compute the bounds of the C.I.\n  point_est &lt;- x/n\n  lb &lt;- point_est - qnorm(1 - alpha / 2) * sqrt(point_est * (1 - point_est)/ n)\n  ub &lt;- point_est + qnorm(1 - alpha / 2) * sqrt(point_est * (1 - point_est)/ n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(point_est, lb, ub)\n  \n  return(out_df)\n}\n\n\n## define parameters to use in our function\nn &lt;- 50 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.48 0.364 0.596\n 2      0.5  0.384 0.616\n 3      0.56 0.445 0.675\n 4      0.3  0.193 0.407\n 5      0.44 0.325 0.555\n 6      0.4  0.286 0.514\n 7      0.6  0.486 0.714\n 8      0.54 0.424 0.656\n 9      0.44 0.325 0.555\n10      0.42 0.305 0.535\n# ℹ 4,990 more rows"
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#step-3-from-the-5000-confidence-intervals-calculate-both-1-the-average-interval-width-and-2-the-coverage-rate.",
    "href": "posts/Mini-Project-3/index.html#step-3-from-the-5000-confidence-intervals-calculate-both-1-the-average-interval-width-and-2-the-coverage-rate.",
    "title": "Mini-Project-3",
    "section": "",
    "text": "## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.230         0.885"
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#step-4-repeat-steps-2-and-3-for-the-5-other-settings-recording-the-average-interval-width-and-the-coverage-rate-for-each-of-these-other-5-settings.",
    "href": "posts/Mini-Project-3/index.html#step-4-repeat-steps-2-and-3-for-the-5-other-settings-recording-the-average-interval-width-and-the-coverage-rate-for-each-of-these-other-5-settings.",
    "title": "Mini-Project-3",
    "section": "",
    "text": "This assumption holds since np &gt; 10 and n(1-p) &gt; 10\n\n50*0.75\n\n[1] 37.5\n\n50*(1-0.75)\n\n[1] 12.5\n\n\n\n## define parameters to use in our function\nn &lt;- 50 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.72 0.616 0.824\n 2      0.74 0.638 0.842\n 3      0.72 0.616 0.824\n 4      0.76 0.661 0.859\n 5      0.7  0.593 0.807\n 6      0.76 0.661 0.859\n 7      0.68 0.571 0.789\n 8      0.76 0.661 0.859\n 9      0.8  0.707 0.893\n10      0.76 0.661 0.859\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.199         0.878\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are both not true\n\n5*0.46\n\n[1] 2.3\n\n5*(1-0.46)\n\n[1] 2.7\n\n\n\n## define parameters to use in our function\nn &lt;- 5 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est      lb    ub\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       0.4  0.0396 0.760\n 2       0.4  0.0396 0.760\n 3       0.6  0.240  0.960\n 4       0.2 -0.0942 0.494\n 5       0.2 -0.0942 0.494\n 6       0.4  0.0396 0.760\n 7       0.2 -0.0942 0.494\n 8       0.4  0.0396 0.760\n 9       0.8  0.506  1.09 \n10       0.6  0.240  0.960\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.630         0.807\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are both not true\n\n5*0.75\n\n[1] 3.75\n\n5*(1-0.75)\n\n[1] 1.25\n\n\n\n## define parameters to use in our function\nn &lt;- 5 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est     lb    ub\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1       1   1      1    \n 2       0.8 0.506  1.09 \n 3       0.8 0.506  1.09 \n 4       0.6 0.240  0.960\n 5       0.8 0.506  1.09 \n 6       0.8 0.506  1.09 \n 7       1   1      1    \n 8       0.8 0.506  1.09 \n 9       0.8 0.506  1.09 \n10       0.4 0.0396 0.760\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.494         0.746\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are not both true\n\n20*0.46\n\n[1] 9.2\n\n20*(1-0.46)\n\n[1] 10.8\n\n\n\n## define parameters to use in our function\nn &lt;- 20 # sample size\np &lt;- 0.46\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.5  0.316 0.684\n 2      0.55 0.367 0.733\n 3      0.35 0.175 0.525\n 4      0.65 0.475 0.825\n 5      0.35 0.175 0.525\n 6      0.55 0.367 0.733\n 7      0.6  0.420 0.780\n 8      0.45 0.267 0.633\n 9      0.5  0.316 0.684\n10      0.35 0.175 0.525\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.357          0.88\n\n\n\n\n\n\nThis assumption does not hold since np &gt; 10 and n(1-p) &gt; 10 are not both true.\n\n20*0.75\n\n[1] 15\n\n20*(1-0.75)\n\n[1] 5\n\n\n\n## define parameters to use in our function\nn &lt;- 20 # sample size\np &lt;- 0.75\nalpha &lt;- 0.1 # used to construct 1 -alpha CI (how much should be in the tails)\n\n## generate 5000 sample proportions and 5000 associated C.I.\nnsim &lt;- 5000 # the number of simulations\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      0.8  0.653 0.947\n 2      0.8  0.653 0.947\n 3      0.8  0.653 0.947\n 4      0.8  0.653 0.947\n 5      0.85 0.719 0.981\n 6      0.65 0.475 0.825\n 7      0.7  0.531 0.869\n 8      0.8  0.653 0.947\n 9      0.55 0.367 0.733\n10      0.7  0.531 0.869\n# ℹ 4,990 more rows\n\n\n\n## this code will calculate both 1) interval width and 2) the coverage rate for all 5000 simulations\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                    ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                           true = 1,\n                                                           false = 0))\n\n\n## this code will calculate both 1) average interval width and 2) average coverage rate for the 5000 simulations\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.308         0.876"
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#table-with-all-of-our-calculations",
    "href": "posts/Mini-Project-3/index.html#table-with-all-of-our-calculations",
    "title": "Mini-Project-3",
    "section": "",
    "text": "Table of Results\n\n\n\n\nn = 5\nn = 20\nn = 50\n\n\n\n\n\\(p = 0.46\\)\nCoverage Rate\n0.8048\n0.8812\n0.8812\n\n\n\\(p = 0.75\\)\nCoverage Rate\n0.7474\n0.8626\n0.8774\n\n\n\n\n\n\n\n\n\n\\(p = 0.46\\)\nAverage Width\n0.6275\n0.3571\n0.2295\n\n\n\\(p = 0.75\\)\nAverage Width\n0.4950\n0.3065\n0.1987"
  },
  {
    "objectID": "posts/Mini-Project-3/index.html#summary-of-our-findings",
    "href": "posts/Mini-Project-3/index.html#summary-of-our-findings",
    "title": "Mini-Project-3",
    "section": "",
    "text": "Our findings can be broken into three different sample size: n = 5, n = 20, and n = 50.\nFirst, n = 5. Whether p = 0.46 or p = 0.75, they both did not hold the “large sample assumption”. After the 5000 simulations, when p = 0.46, we got an average width of 0.6275 and coverage rate of 0.8048. We are not surprised that the average width is big because our n is so small. Our converge rate is roughly about 0.80, which is not what we wanted since we are doing a 90% C.I.This can be explained because with such a small n, our phats are ending up outside our confidence interval. When p = 0.75, we got an average width of 0.4950 and a coverage rate of 0.7474. Again, we are not surprised by the big interval and smaller than expected coverage rate due to the fact we have a small n and did not hold the “large sample assumption”.\nWhen n = 20, we still saw that it didn’t matter what our p was. Whether it was 0.46 or 0.75, the “large sample assumption” still does not hold. After the 5000 simulations, when p = 0.46, the average width was 0.3571 and coverage rate of 0.8812. We notice that our average width of the confidence interval has decreased from n = 5, which makes sense as a larger sample size should give us a smaller confidence interval. We also notice that our coverage rate was closer to the 0.9 we are looking for. This coverage rate was much closer due to the fact that one of the two test did pass, with one of the test being very close to passing. When p = 0.75, we get an average width of 0.3065 and a coverage rate of 0.8626. Again, like the p = 0.46, we noticed improved numbers as our confidence interval was smaller and our coverage rate was much closer to 0.9 then when n = 5.\nFinally, when n = 50, we saw that when our p = 0.46 or p = 0.75, the “large sample assumption” passed. When p = 0.46, we got an average width of 0.2295 and a coverage rate of 0.8812. We notice again as we increased n, we obtain an even smaller average width and a coverage rate of almost 0.90 This is great as this is what we expect when trying to take a 90% C.I. We get very similar results when p = 0.75 as average width was 0.1987 (the smallest out of all 6 examples) and a coverage rate of 0.8774. This was the smallest average width, which was not suprising as n = 50. This was also again a very close coverage rate to 0.90 as both “large sample assumptions passed”.\nOverall, the biggest take away from our simulations is we get the smallest average width of an interval and best coverage rate when we have a large n and pass the “large sample assumption”."
  },
  {
    "objectID": "posts/Mini-Project-5/index.html",
    "href": "posts/Mini-Project-5/index.html",
    "title": "Mini Project 5",
    "section": "",
    "text": "“All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project.” ~ Clayton Fogler\nQuestions:\n\nTowards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\n\n\nI think what the author is trying to get at her is instead of viewing p = 0.049 and p 0.051 differently, statisticians will use more statistical thinking to come up with their conclusions. This means using variability, prior knowledge, and other information to come up with conclusions instead of just looking at a p value. Some examples that embody “statistical thinking” include using confidence intervals to also assess the problem, looking at the limitations of the data we have gathered, and looking at the real life impact our findings may have instead of just making a judgment based off a p value.\n\n\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\n\n\nThe author is trying to say is that taking a number such as a p-value and just assigning it to either statistically significant or not statistically significant is oversimplifying what the p-value stands for. In a situation where you get p = 0.049, since it is &lt; 0.05, you would report the p value as being statistically significant, even though it is so close to the cut line. On the other hand, if you got a p value of 0.051, you are technically over the 0.05 threshold and therefore not statistically significant. In other words, you get this dichotomization where we are deciding statistical significance based on a 1000th decimal point. Instead, we should just look at the p value and judge the results from there instead of splitting into significant or not significant.\n\n\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\n\nYes, I agree. You should not let a p value of 0.051 vs a p value of 0.049 be the difference whether you present your findings. Sort of like question two, we shouldn’t let a threshold withhold us from presenting what could still be very useful findings. Instead, deciding whether or not to present or highlight results should be based off statistical thinking. Use other statistics such as confidence intervals that can still provide useful findings.\n\n\nSection 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\n\n\nI 100 percent agree. If one person is trying to test a new medicine for a cure and another person is trying to test whether a school’s favorite color is anything different than red, they should not be using the same approach to statistical inference. One situation could be life and death and needs a ton more attention while the other situation is just dealing with what a person’s favorite color is. To use a single one-size-fits-all approach would be irresponsible for statistical inference.\n\n\nSection 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\n\n\nI would say statistical thoughtfulness refers to thinking outside of the box. Instead of going into research with a goal in mind of finding a specific result, you approach your topic from all angles using multiple different tests to take out any bias you may have towards the results and instead let the results speak for themselves. For example, if I was an analyst for a sports team, I wouldn’t just look for results that are in our favor, I would make sure to use a rounded approach and get all positive and negative results to make my assumptions.\n\n\nSection 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\n\n\nI think that the authors believe there is a misunderstanding when statisticians use words such as significance and confidence. When a non-statistics person hears the word significance, they kind of just assume that it means the truth or real, but it really only means there is evidence that it could be true, not that it is. The same can be said about confidence. When a non-statistician hears the 95% confidence interval of Aaron Judges batting average is (0.314, 0.421), they assume that it means there is a 95% probability that Aaron Judge’s batting average is between 0.314 and 0.421. However, the probability of Aaron Judges batting average being between 0.314 and 0.421 is either 0 or 1 (it is either in between or not). Instead, what a confidence interval says is we are 95% confident that the true mean of Aaron Judge’s batting average is between 0.314 and 0.421. I do agree that there is this issue, and it is misleading, however, I don’t think that just inventing a new term called compatibility makes it better. Instead, there needs to be an emphasis on learning how to share statistical research with those not fully knowledgeable about statistical terms and references. Creating a new term for non-statistical people to misunderstand is not a solution.\n\n\nFind a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\n\n\nA quote that stands out is “We summarize our recommendations in two sentences totaling seven words: ‘Accept uncertainty. Be thoughtful, open, and modest.’ Remember ‘ATOM’” (Section 3, bottom right of page 2). The reason this quote stands out to me is because this one quote really captures what the whole reading is about. Don’t have this dichotomized thought process where the answer is either a or b. Accept that there will be uncertainty in what the answer may be. Look at the problem from all angles. Be open to being wrong or right. If I forget everything I read today, I hope I can at least remember “ATOM”."
  }
]